"""
    è¯¥æ–‡ä»¶ä¸­ä¸»è¦åŒ…å«ä¸‰ä¸ªå‡½æ•°

    ä¸å…·å¤‡å¤šçº¿ç¨‹èƒ½åŠ›çš„å‡½æ•°ï¼š
    1. predict: æ­£å¸¸å¯¹è¯æ—¶ä½¿ç”¨ï¼Œå…·å¤‡å®Œå¤‡çš„äº¤äº’åŠŸèƒ½ï¼Œä¸å¯å¤šçº¿ç¨‹

    å…·å¤‡å¤šçº¿ç¨‹è°ƒç”¨èƒ½åŠ›çš„å‡½æ•°
    2. predict_no_ui_long_connectionï¼šæ”¯æŒå¤šçº¿ç¨‹
"""

import json
import os
import re
import time
import traceback
import requests
import random

from loguru import logger

# config_private.pyæ”¾è‡ªå·±çš„ç§˜å¯†å¦‚APIå’Œä»£ç†ç½‘å€
# è¯»å–æ—¶é¦–å…ˆçœ‹æ˜¯å¦å­˜åœ¨ç§å¯†çš„config_privateé…ç½®æ–‡ä»¶ï¼ˆä¸å—gitç®¡æ§ï¼‰ï¼Œå¦‚æœæœ‰ï¼Œåˆ™è¦†ç›–åŸconfigæ–‡ä»¶
from toolbox import get_conf, update_ui, is_any_api_key, select_api_key, what_keys, clip_history
from toolbox import trimmed_format_exc, is_the_upload_folder, read_one_api_model_name, log_chat
from toolbox import ChatBotWithCookies, have_any_recent_upload_image_files, encode_image
proxies, WHEN_TO_USE_PROXY, TIMEOUT_SECONDS, MAX_RETRY, API_ORG, AZURE_CFG_ARRAY = \
    get_conf('proxies', 'WHEN_TO_USE_PROXY', 'TIMEOUT_SECONDS', 'MAX_RETRY', 'API_ORG', 'AZURE_CFG_ARRAY')

if "Connect_OpenAI" not in WHEN_TO_USE_PROXY:
    if proxies is not None:
        logger.error("è™½ç„¶æ‚¨é…ç½®äº†ä»£ç†è®¾ç½®ï¼Œä½†ä¸ä¼šåœ¨è¿æ¥OpenAIçš„è¿‡ç¨‹ä¸­èµ·ä½œç”¨ï¼Œè¯·æ£€æŸ¥WHEN_TO_USE_PROXYé…ç½®ã€‚")
        proxies = None

timeout_bot_msg = '[Local Message] Request timeout. Network error. Please check proxy settings in config.py.' + \
                  'ç½‘ç»œé”™è¯¯ï¼Œæ£€æŸ¥ä»£ç†æœåŠ¡å™¨æ˜¯å¦å¯ç”¨ï¼Œä»¥åŠä»£ç†è®¾ç½®çš„æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œæ ¼å¼é¡»æ˜¯[åè®®]://[åœ°å€]:[ç«¯å£]ï¼Œç¼ºä¸€ä¸å¯ã€‚'

def get_full_error(chunk, stream_response):
    """
        è·å–å®Œæ•´çš„ä»Openaiè¿”å›çš„æŠ¥é”™
    """
    while True:
        try:
            chunk += next(stream_response)
        except:
            break
    return chunk

def make_multimodal_input(inputs, image_paths):
    image_base64_array = []
    for image_path in image_paths:
        path = os.path.abspath(image_path)
        base64 = encode_image(path)
        inputs = inputs + f'<br/><br/><div align="center"><img src="file={path}" base64="{base64}"></div>'
        image_base64_array.append(base64)
    return inputs, image_base64_array

def reverse_base64_from_input(inputs):
    # å®šä¹‰ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é… Base64 å­—ç¬¦ä¸²ï¼ˆå‡è®¾æ ¼å¼ä¸º base64="<Base64ç¼–ç >"ï¼‰
    # pattern = re.compile(r'base64="([^"]+)"></div>')
    pattern = re.compile(r'<br/><br/><div align="center"><img[^<>]+base64="([^"]+)"></div>')
    # ä½¿ç”¨ findall æ–¹æ³•æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„ Base64 å­—ç¬¦ä¸²
    base64_strings = pattern.findall(inputs)
    # è¿”å›åè½¬åçš„ Base64 å­—ç¬¦ä¸²åˆ—è¡¨
    return base64_strings

def contain_base64(inputs):
    base64_strings = reverse_base64_from_input(inputs)
    return len(base64_strings) > 0

def append_image_if_contain_base64(inputs):
    if not contain_base64(inputs):
        return inputs
    else:
        image_base64_array = reverse_base64_from_input(inputs)
        pattern = re.compile(r'<br/><br/><div align="center"><img[^><]+></div>')
        inputs = re.sub(pattern, '', inputs)
        res = []
        res.append({
            "type": "text",
            "text": inputs
        })
        for image_base64 in image_base64_array:
            res.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{image_base64}"
                }
            })
        return res

def remove_image_if_contain_base64(inputs):
    if not contain_base64(inputs):
        return inputs
    else:
        pattern = re.compile(r'<br/><br/><div align="center"><img[^><]+></div>')
        inputs = re.sub(pattern, '', inputs)
        return inputs

def decode_chunk(chunk):
    # æå‰è¯»å–ä¸€äº›ä¿¡æ¯ ï¼ˆç”¨äºåˆ¤æ–­å¼‚å¸¸ï¼‰
    chunk_decoded = chunk.decode()
    chunkjson = None
    has_choices = False
    choice_valid = False
    has_content = False
    has_role = False
    try:
        chunkjson = json.loads(chunk_decoded[6:])
        has_choices = 'choices' in chunkjson
        if has_choices: choice_valid = (len(chunkjson['choices']) > 0)
        if has_choices and choice_valid: has_content = ("content" in chunkjson['choices'][0]["delta"])
        if has_content: has_content = (chunkjson['choices'][0]["delta"]["content"] is not None)
        if has_choices and choice_valid: has_role = "role" in chunkjson['choices'][0]["delta"]
    except:
        pass
    return chunk_decoded, chunkjson, has_choices, choice_valid, has_content, has_role

from functools import lru_cache
@lru_cache(maxsize=32)
def verify_endpoint(endpoint):
    """
        æ£€æŸ¥endpointæ˜¯å¦å¯ç”¨
    """
    if "ä½ äº²æ‰‹å†™çš„apiåç§°" in endpoint:
        raise ValueError("Endpointä¸æ­£ç¡®, è¯·æ£€æŸ¥AZURE_ENDPOINTçš„é…ç½®! å½“å‰çš„Endpointä¸º:" + endpoint)
    return endpoint

def predict_no_ui_long_connection(inputs:str, llm_kwargs:dict, history:list=[], sys_prompt:str="", observe_window:list=None, console_silence:bool=False):
    """
    å‘é€è‡³chatGPTï¼Œç­‰å¾…å›å¤ï¼Œä¸€æ¬¡æ€§å®Œæˆï¼Œä¸æ˜¾ç¤ºä¸­é—´è¿‡ç¨‹ã€‚ä½†å†…éƒ¨ç”¨streamçš„æ–¹æ³•é¿å…ä¸­é€”ç½‘çº¿è¢«æã€‚
    inputsï¼š
        æ˜¯æœ¬æ¬¡é—®è¯¢çš„è¾“å…¥
    sys_prompt:
        ç³»ç»Ÿé™é»˜prompt
    llm_kwargsï¼š
        chatGPTçš„å†…éƒ¨è°ƒä¼˜å‚æ•°
    historyï¼š
        æ˜¯ä¹‹å‰çš„å¯¹è¯åˆ—è¡¨
    observe_window = Noneï¼š
        ç”¨äºè´Ÿè´£è·¨è¶Šçº¿ç¨‹ä¼ é€’å·²ç»è¾“å‡ºçš„éƒ¨åˆ†ï¼Œå¤§éƒ¨åˆ†æ—¶å€™ä»…ä»…ä¸ºäº†fancyçš„è§†è§‰æ•ˆæœï¼Œç•™ç©ºå³å¯ã€‚observe_window[0]ï¼šè§‚æµ‹çª—ã€‚observe_window[1]ï¼šçœ‹é—¨ç‹—
    """
    from request_llms.bridge_all import model_info

    watch_dog_patience = 1500 # çœ‹é—¨ç‹—çš„è€å¿ƒ, è®¾ç½®5ç§’å³å¯

    if model_info[llm_kwargs['llm_model']].get('openai_disable_stream', False): stream = False
    else: stream = True

    headers, payload = generate_payload(inputs, llm_kwargs, history, system_prompt=sys_prompt, stream=stream)
    retry = 0
    while True:
        try:
            # make a POST request to the API endpoint, stream=False
            endpoint = verify_endpoint(model_info[llm_kwargs['llm_model']]['endpoint'])
            response = requests.post(endpoint, headers=headers, proxies=proxies,
                                    json=payload, stream=stream, timeout=TIMEOUT_SECONDS); break
        except requests.exceptions.ReadTimeout as e:
            retry += 1
            traceback.print_exc()
            if retry > MAX_RETRY: raise TimeoutError
            if MAX_RETRY!=0: logger.error(f'è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯• ({retry}/{MAX_RETRY}) â€¦â€¦')

    if not stream:
        # è¯¥åˆ†æ”¯ä»…é€‚ç”¨äºä¸æ”¯æŒstreamçš„o1æ¨¡å‹ï¼Œå…¶ä»–æƒ…å½¢ä¸€å¾‹ä¸é€‚ç”¨
        chunkjson = json.loads(response.content.decode())
        gpt_replying_buffer = chunkjson['choices'][0]["message"]["content"]
        return gpt_replying_buffer

    stream_response = response.iter_lines()
    result = ''
    json_data = None
    while True:
        try: chunk = next(stream_response)
        except StopIteration:
            break
        except requests.exceptions.ConnectionError:
            chunk = next(stream_response) # å¤±è´¥äº†ï¼Œé‡è¯•ä¸€æ¬¡ï¼Ÿå†å¤±è´¥å°±æ²¡åŠæ³•äº†ã€‚
        chunk_decoded, chunkjson, has_choices, choice_valid, has_content, has_role = decode_chunk(chunk)
        if len(chunk_decoded)==0: continue
        if not chunk_decoded.startswith('data:'):
            error_msg = get_full_error(chunk, stream_response).decode()
            if "reduce the length" in error_msg:
                raise ConnectionAbortedError("OpenAIæ‹’ç»äº†è¯·æ±‚:" + error_msg)
            elif """type":"upstream_error","param":"307""" in error_msg:
                raise ConnectionAbortedError("æ­£å¸¸ç»“æŸï¼Œä½†æ˜¾ç¤ºTokenä¸è¶³ï¼Œå¯¼è‡´è¾“å‡ºä¸å®Œæ•´ï¼Œè¯·å‰Šå‡å•æ¬¡è¾“å…¥çš„æ–‡æœ¬é‡ã€‚")
            else:
                raise RuntimeError("OpenAIæ‹’ç»äº†è¯·æ±‚ï¼š" + error_msg)
        if ('data: [DONE]' in chunk_decoded): break # api2d & aioagi æ­£å¸¸å®Œæˆ
        # æå‰è¯»å–ä¸€äº›ä¿¡æ¯ ï¼ˆç”¨äºåˆ¤æ–­å¼‚å¸¸ï¼‰
        if has_choices and not choice_valid:
            # ä¸€äº›åƒåœ¾ç¬¬ä¸‰æ–¹æ¥å£çš„å‡ºç°è¿™æ ·çš„é”™è¯¯
            continue
        json_data = chunkjson['choices'][0]
        delta = json_data["delta"]

        if len(delta) == 0:
            is_termination_certain = False
            if (has_choices) and (chunkjson['choices'][0].get('finish_reason', 'null') == 'stop'): is_termination_certain = True
            if is_termination_certain: break
            else: continue # å¯¹äºä¸ç¬¦åˆè§„èŒƒçš„ç‹—å±æ¥å£ï¼Œè¿™é‡Œéœ€è¦ç»§ç»­

        if (not has_content) and has_role: continue
        if (not has_content) and (not has_role): continue # raise RuntimeError("å‘ç°ä¸æ ‡å‡†çš„ç¬¬ä¸‰æ–¹æ¥å£ï¼š"+delta)
        if has_content: # has_role = True/False
            result += delta["content"]
            if not console_silence: print(delta["content"], end='')
            if observe_window is not None:
                # è§‚æµ‹çª—ï¼ŒæŠŠå·²ç»è·å–çš„æ•°æ®æ˜¾ç¤ºå‡ºå»
                if len(observe_window) >= 1:
                    observe_window[0] += delta["content"]
                # çœ‹é—¨ç‹—ï¼Œå¦‚æœè¶…è¿‡æœŸé™æ²¡æœ‰å–‚ç‹—ï¼Œåˆ™ç»ˆæ­¢
                if len(observe_window) >= 2:
                    if (time.time()-observe_window[1]) > watch_dog_patience:
                        raise RuntimeError("ç”¨æˆ·å–æ¶ˆäº†ç¨‹åºã€‚")
        else: raise RuntimeError("æ„å¤–Jsonç»“æ„ï¼š"+delta)

    finish_reason = json_data.get('finish_reason', None) if json_data else None
    if finish_reason == 'content_filter':
        raise RuntimeError("ç”±äºæé—®å«ä¸åˆè§„å†…å®¹è¢«è¿‡æ»¤ã€‚")
    if finish_reason == 'length':
        raise ConnectionAbortedError("æ­£å¸¸ç»“æŸï¼Œä½†æ˜¾ç¤ºTokenä¸è¶³ï¼Œå¯¼è‡´è¾“å‡ºä¸å®Œæ•´ï¼Œè¯·å‰Šå‡å•æ¬¡è¾“å…¥çš„æ–‡æœ¬é‡ã€‚")

    return result


def predict(inputs:str, llm_kwargs:dict, plugin_kwargs:dict, chatbot:ChatBotWithCookies,
            history:list=[], system_prompt:str='', stream:bool=True, additional_fn:str=None):
    """
    å‘é€è‡³chatGPTï¼Œæµå¼è·å–è¾“å‡ºã€‚
    ç”¨äºåŸºç¡€çš„å¯¹è¯åŠŸèƒ½ã€‚
    inputs æ˜¯æœ¬æ¬¡é—®è¯¢çš„è¾“å…¥
    top_p, temperatureæ˜¯chatGPTçš„å†…éƒ¨è°ƒä¼˜å‚æ•°
    history æ˜¯ä¹‹å‰çš„å¯¹è¯åˆ—è¡¨ï¼ˆæ³¨æ„æ— è®ºæ˜¯inputsè¿˜æ˜¯historyï¼Œå†…å®¹å¤ªé•¿äº†éƒ½ä¼šè§¦å‘tokenæ•°é‡æº¢å‡ºçš„é”™è¯¯ï¼‰
    chatbot ä¸ºWebUIä¸­æ˜¾ç¤ºçš„å¯¹è¯åˆ—è¡¨ï¼Œä¿®æ”¹å®ƒï¼Œç„¶åyieldå‡ºå»ï¼Œå¯ä»¥ç›´æ¥ä¿®æ”¹å¯¹è¯ç•Œé¢å†…å®¹
    additional_fnä»£è¡¨ç‚¹å‡»çš„å“ªä¸ªæŒ‰é’®ï¼ŒæŒ‰é’®è§functional.py
    """
    from request_llms.bridge_all import model_info
    if is_any_api_key(inputs):
        chatbot._cookies['api_key'] = inputs
        chatbot.append(("è¾“å…¥å·²è¯†åˆ«ä¸ºopenaiçš„api_key", what_keys(inputs)))
        yield from update_ui(chatbot=chatbot, history=history, msg="api_keyå·²å¯¼å…¥") # åˆ·æ–°ç•Œé¢
        return
    elif not is_any_api_key(chatbot._cookies['api_key']):
        chatbot.append((inputs, "ç¼ºå°‘api_keyã€‚\n\n1. ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼šç›´æ¥åœ¨è¾“å…¥åŒºé”®å…¥api_keyï¼Œç„¶åå›è½¦æäº¤ã€‚\n\n2. é•¿æ•ˆè§£å†³æ–¹æ¡ˆï¼šåœ¨config.pyä¸­é…ç½®ã€‚"))
        yield from update_ui(chatbot=chatbot, history=history, msg="ç¼ºå°‘api_key") # åˆ·æ–°ç•Œé¢
        return

    user_input = inputs
    if additional_fn is not None:
        from core_functional import handle_core_functionality
        inputs, history = handle_core_functionality(additional_fn, inputs, history, chatbot)

    # å¤šæ¨¡æ€æ¨¡å‹
    has_multimodal_capacity = model_info[llm_kwargs['llm_model']].get('has_multimodal_capacity', False)
    if has_multimodal_capacity:
        has_recent_image_upload, image_paths = have_any_recent_upload_image_files(chatbot, pop=True)
    else:
        has_recent_image_upload, image_paths = False, []
    if has_recent_image_upload:
        _inputs, image_base64_array = make_multimodal_input(inputs, image_paths)
    else:
        _inputs, image_base64_array = inputs, []
    chatbot.append((_inputs, ""))
    yield from update_ui(chatbot=chatbot, history=history, msg="ç­‰å¾…å“åº”") # åˆ·æ–°ç•Œé¢

    # ç¦ç”¨streamçš„ç‰¹æ®Šæ¨¡å‹å¤„ç†
    if model_info[llm_kwargs['llm_model']].get('openai_disable_stream', False): stream = False
    else: stream = True

    # check mis-behavior
    if is_the_upload_folder(user_input):
        chatbot[-1] = (inputs, f"[Local Message] æ£€æµ‹åˆ°æ“ä½œé”™è¯¯ï¼å½“æ‚¨ä¸Šä¼ æ–‡æ¡£ä¹‹åï¼Œéœ€ç‚¹å‡»â€œ**å‡½æ•°æ’ä»¶åŒº**â€æŒ‰é’®è¿›è¡Œå¤„ç†ï¼Œè¯·å‹¿ç‚¹å‡»â€œæäº¤â€æŒ‰é’®æˆ–è€…â€œåŸºç¡€åŠŸèƒ½åŒºâ€æŒ‰é’®ã€‚")
        yield from update_ui(chatbot=chatbot, history=history, msg="æ­£å¸¸") # åˆ·æ–°ç•Œé¢
        time.sleep(2)

    try:
        headers, payload = generate_payload(inputs, llm_kwargs, history, system_prompt, image_base64_array, has_multimodal_capacity, stream)
    except RuntimeError as e:
        chatbot[-1] = (inputs, f"æ‚¨æä¾›çš„api-keyä¸æ»¡è¶³è¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•å¯ç”¨äº{llm_kwargs['llm_model']}çš„api-keyã€‚æ‚¨å¯èƒ½é€‰æ‹©äº†é”™è¯¯çš„æ¨¡å‹æˆ–è¯·æ±‚æºã€‚")
        yield from update_ui(chatbot=chatbot, history=history, msg="api-keyä¸æ»¡è¶³è¦æ±‚") # åˆ·æ–°ç•Œé¢
        return

    # æ£€æŸ¥endpointæ˜¯å¦åˆæ³•
    try:
        endpoint = verify_endpoint(model_info[llm_kwargs['llm_model']]['endpoint'])
    except:
        tb_str = '```\n' + trimmed_format_exc() + '```'
        chatbot[-1] = (inputs, tb_str)
        yield from update_ui(chatbot=chatbot, history=history, msg="Endpointä¸æ»¡è¶³è¦æ±‚") # åˆ·æ–°ç•Œé¢
        return

    # åŠ å…¥å†å²
    if has_recent_image_upload:
        history.extend([_inputs, ""])
    else:
        history.extend([inputs, ""])

    previous_ui_reflesh_time = 0
    ui_reflesh_min_interval = 0.0

    # ä½¿ç”¨çº¿ç¨‹åˆ›å»ºèŠå¤©ï¼Œä¸»çº¿ç¨‹è´Ÿè´£æ›´æ–°è¿›åº¦æ˜¾ç¤º
    import time
    import threading

    start_time = time.time()

    response = None
    creation_complete = False
    creation_error = None
    max_wait_time = 600  # æœ€å¤šç­‰å¾…60ç§’

    def create_post():
        nonlocal response, creation_complete, creation_error
        try:
            response = requests.post(endpoint, headers=headers, proxies=proxies,
                                     json=payload, stream=stream, timeout=TIMEOUT_SECONDS)
            creation_complete = True
        except Exception as e:
            print(f"POSTå¤±è´¥: {e}")
            creation_error = e
            creation_complete = True

    # å¯åŠ¨POSTèŠå¤©åˆ›å»ºçº¿ç¨‹
    creation_thread = threading.Thread(target=create_post, daemon=True)
    creation_thread.start()

    # ä¸»çº¿ç¨‹æ¯ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦æ˜¾ç¤º
    while not creation_complete and (time.time() - start_time) < max_wait_time:
        elapsed = int(time.time() - start_time)

        progress_msg = f"â³ è¯·ç¨å€™...({elapsed}s)"

        chatbot[-1] = [chatbot[-1][0], progress_msg]

        # æ›´æ–°UI
        for ui_update in update_ui(chatbot=chatbot, history=history):
            yield ui_update

        # ç­‰å¾…5ç§’æˆ–ç›´åˆ°åˆ›å»ºå®Œæˆ
        wait_start = time.time()
        while (time.time() - wait_start) < 1 and not creation_complete:
            time.sleep(0.5)  # æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦å®Œæˆ

    # å¦‚æœåˆ›å»ºçº¿ç¨‹ä»åœ¨è¿è¡Œä¸”è¶…æ—¶ï¼Œæ˜¾ç¤ºè¶…æ—¶ä¿¡æ¯
    if not creation_complete:
        print("POSTåˆ›å»ºè¶…æ—¶")
        progress_msg = f"ğŸ¤–æ¨¡å‹è¿”å›è¶…æ—¶ï¼Œè¯·é‡è¯•/è”ç³»ç®¡ç†å‘˜ç»´æŠ¤ï¼"
        chatbot[-1] = [chatbot[-1][0], progress_msg]
        for ui_update in update_ui(chatbot=chatbot, history=history):
            yield ui_update
        raise Exception("POSTåˆ›å»ºè¶…æ—¶ï¼Œè¯·é‡è¯•")

    # å¦‚æœåˆ›å»ºè¿‡ç¨‹ä¸­æœ‰é”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸
    if creation_error:
        progress_msg = f"ğŸ¤–æ¨¡å‹è¿”å›å‡ºé”™ï¼Œè¯·é‡è¯•/è”ç³»ç®¡ç†å‘˜ç»´æŠ¤ï¼"
        chatbot[-1] = [chatbot[-1][0], progress_msg]
        for ui_update in update_ui(chatbot=chatbot, history=history):
            yield ui_update
        raise creation_error

    if not stream:
        # è¯¥åˆ†æ”¯ä»…é€‚ç”¨äºä¸æ”¯æŒstreamçš„o1æ¨¡å‹ï¼Œå…¶ä»–æƒ…å½¢ä¸€å¾‹ä¸é€‚ç”¨
        yield from handle_o1_model_special(response, inputs, llm_kwargs, chatbot, history)
        return

    if stream:
        reach_termination = False   # å¤„ç†ä¸€äº› new-api çš„å¥‡è‘©å¼‚å¸¸
        gpt_replying_buffer = ""
        is_head_of_the_stream = True
        stream_response =  response.iter_lines()
        while True:
            try:
                chunk = next(stream_response)
            except StopIteration:
                # éOpenAIå®˜æ–¹æ¥å£çš„å‡ºç°è¿™æ ·çš„æŠ¥é”™ï¼ŒOpenAIå’ŒAPI2Dä¸ä¼šèµ°è¿™é‡Œ
                chunk_decoded = chunk.decode()
                error_msg = chunk_decoded
                # é¦–å…ˆæ’é™¤ä¸€ä¸ªone-apiæ²¡æœ‰doneæ•°æ®åŒ…çš„ç¬¬ä¸‰æ–¹Bugæƒ…å½¢
                if len(gpt_replying_buffer.strip()) > 0 and len(error_msg) == 0:
                    yield from update_ui(chatbot=chatbot, history=history, msg="æ£€æµ‹åˆ°æœ‰ç¼ºé™·çš„æ¥å£ï¼Œå»ºè®®é€‰æ‹©æ›´ç¨³å®šçš„æ¥å£ã€‚")
                    if not reach_termination:
                        reach_termination = True
                        log_chat(llm_model=llm_kwargs["llm_model"], input_str=inputs, output_str=gpt_replying_buffer)
                    break
                # å…¶ä»–æƒ…å†µï¼Œç›´æ¥è¿”å›æŠ¥é”™
                chatbot, history = handle_error(inputs, llm_kwargs, chatbot, history, chunk_decoded, error_msg)
                yield from update_ui(chatbot=chatbot, history=history, msg="æ¥å£è¿”å›äº†é”™è¯¯:" + chunk.decode()) # åˆ·æ–°ç•Œé¢
                return

            # æå‰è¯»å–ä¸€äº›ä¿¡æ¯ ï¼ˆç”¨äºåˆ¤æ–­å¼‚å¸¸ï¼‰
            chunk_decoded, chunkjson, has_choices, choice_valid, has_content, has_role = decode_chunk(chunk)

            if is_head_of_the_stream and (r'"object":"error"' not in chunk_decoded) and (r"content" not in chunk_decoded):
                # æ•°æ®æµçš„ç¬¬ä¸€å¸§ä¸æºå¸¦content
                is_head_of_the_stream = False; continue

            if "error" in chunk_decoded: logger.error(f"æ¥å£è¿”å›äº†æœªçŸ¥é”™è¯¯: {chunk_decoded}")

            if chunk:
                try:
                    if has_choices and not choice_valid:
                        # ä¸€äº›åƒåœ¾ç¬¬ä¸‰æ–¹æ¥å£çš„å‡ºç°è¿™æ ·çš„é”™è¯¯
                        continue
                    if ('data: [DONE]' not in chunk_decoded) and len(chunk_decoded) > 0 and (chunkjson is None):
                        # ä¼ é€’è¿›æ¥ä¸€äº›å¥‡æ€ªçš„ä¸œè¥¿
                        raise ValueError(f'æ— æ³•è¯»å–ä»¥ä¸‹æ•°æ®ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚\n\n{chunk_decoded}')
                    # å‰è€…æ˜¯API2D & One-APIçš„ç»“æŸæ¡ä»¶ï¼Œåè€…æ˜¯OPENAIçš„ç»“æŸæ¡ä»¶
                    one_api_terminate = ('data: [DONE]' in chunk_decoded)
                    openai_terminate = (has_choices) and (len(chunkjson['choices'][0]["delta"]) == 0)
                    if one_api_terminate or openai_terminate:
                        is_termination_certain = False
                        if one_api_terminate: is_termination_certain = True # æŠ“å–ç¬¦åˆè§„èŒƒçš„ç»“æŸæ¡ä»¶
                        elif (has_choices) and (chunkjson['choices'][0].get('finish_reason', 'null') == 'stop'): is_termination_certain = True # æŠ“å–ç¬¦åˆè§„èŒƒçš„ç»“æŸæ¡ä»¶
                        if is_termination_certain:
                            reach_termination = True
                            log_chat(llm_model=llm_kwargs["llm_model"], input_str=inputs, output_str=gpt_replying_buffer)
                            break # å¯¹äºç¬¦åˆè§„èŒƒçš„æ¥å£ï¼Œè¿™é‡Œå¯ä»¥break
                        else:
                            continue # å¯¹äºä¸ç¬¦åˆè§„èŒƒçš„æ¥å£ï¼Œè¿™é‡Œéœ€è¦ç»§ç»­
                    # åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»å¯ä»¥å‡å®šå¿…é¡»åŒ…å«choiceäº†
                    try:
                        status_text = f"finish_reason: {chunkjson['choices'][0].get('finish_reason', 'null')}"
                    except:
                        logger.error(f"ä¸€äº›ç¬¬ä¸‰æ–¹æ¥å£å‡ºç°è¿™æ ·çš„é”™è¯¯ï¼Œå…¼å®¹ä¸€ä¸‹å§: {chunk_decoded}")
                    # å¤„ç†æ•°æ®æµçš„ä¸»ä½“
                    if has_content:
                        # æ­£å¸¸æƒ…å†µ
                        gpt_replying_buffer = gpt_replying_buffer + chunkjson['choices'][0]["delta"]["content"]
                    elif has_role:
                        # ä¸€äº›ç¬¬ä¸‰æ–¹æ¥å£çš„å‡ºç°è¿™æ ·çš„é”™è¯¯ï¼Œå…¼å®¹ä¸€ä¸‹å§
                        continue
                    else:
                        # è‡³æ­¤å·²ç»è¶…å‡ºäº†æ­£å¸¸æ¥å£åº”è¯¥è¿›å…¥çš„èŒƒå›´ï¼Œä¸€äº›ç¬¬ä¸‰æ–¹æ¥å£ä¼šå‡ºç°è¿™æ ·çš„é”™è¯¯
                        if chunkjson['choices'][0]["delta"].get("content", None) is None:
                            logger.error(f"ä¸€äº›ç¬¬ä¸‰æ–¹æ¥å£å‡ºç°è¿™æ ·çš„é”™è¯¯ï¼Œå…¼å®¹ä¸€ä¸‹å§: {chunk_decoded}")
                            continue
                        gpt_replying_buffer = gpt_replying_buffer + chunkjson['choices'][0]["delta"]["content"]

                    history[-1] = gpt_replying_buffer
                    chatbot[-1] = (history[-2], history[-1])
                    if time.time() - previous_ui_reflesh_time > ui_reflesh_min_interval:
                        yield from update_ui(chatbot=chatbot, history=history, msg=status_text) # åˆ·æ–°ç•Œé¢
                        previous_ui_reflesh_time = time.time()
                except Exception as e:
                    yield from update_ui(chatbot=chatbot, history=history, msg="Jsonè§£æä¸åˆå¸¸è§„") # åˆ·æ–°ç•Œé¢
                    chunk = get_full_error(chunk, stream_response)
                    chunk_decoded = chunk.decode()
                    error_msg = chunk_decoded
                    chatbot, history = handle_error(inputs, llm_kwargs, chatbot, history, chunk_decoded, error_msg)
                    logger.error(error_msg)
                    yield from update_ui(chatbot=chatbot, history=history, msg="Jsonè§£æå¼‚å¸¸" + error_msg) # åˆ·æ–°ç•Œé¢
                    return
        yield from update_ui(chatbot=chatbot, history=history, msg="å®Œæˆ") # åˆ·æ–°ç•Œé¢
        return  # return from stream-branch

def handle_o1_model_special(response, inputs, llm_kwargs, chatbot, history):
    try:
        chunkjson = json.loads(response.content.decode())
        gpt_replying_buffer = chunkjson['choices'][0]["message"]["content"]
        log_chat(llm_model=llm_kwargs["llm_model"], input_str=inputs, output_str=gpt_replying_buffer)
        history[-1] = gpt_replying_buffer
        chatbot[-1] = (history[-2], history[-1])
        yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢
    except Exception as e:
        yield from update_ui(chatbot=chatbot, history=history, msg="Jsonè§£æå¼‚å¸¸" + response.text) # åˆ·æ–°ç•Œé¢

def handle_error(inputs, llm_kwargs, chatbot, history, chunk_decoded, error_msg):
    from request_llms.bridge_all import model_info
    openai_website = ' è¯·ç™»å½•OpenAIæŸ¥çœ‹è¯¦æƒ… https://platform.openai.com/signup'
    if "reduce the length" in error_msg:
        if len(history) >= 2: history[-1] = ""; history[-2] = "" # æ¸…é™¤å½“å‰æº¢å‡ºçš„è¾“å…¥ï¼šhistory[-2] æ˜¯æœ¬æ¬¡è¾“å…¥, history[-1] æ˜¯æœ¬æ¬¡è¾“å‡º
        history = clip_history(inputs=inputs, history=history, tokenizer=model_info[llm_kwargs['llm_model']]['tokenizer'],
                                               max_token_limit=(model_info[llm_kwargs['llm_model']]['max_token'])) # historyè‡³å°‘é‡Šæ”¾äºŒåˆ†ä¹‹ä¸€
        chatbot[-1] = (chatbot[-1][0], "[Local Message] Reduce the length. æœ¬æ¬¡è¾“å…¥è¿‡é•¿, æˆ–å†å²æ•°æ®è¿‡é•¿. å†å²ç¼“å­˜æ•°æ®å·²éƒ¨åˆ†é‡Šæ”¾, æ‚¨å¯ä»¥è¯·å†æ¬¡å°è¯•. (è‹¥å†æ¬¡å¤±è´¥åˆ™æ›´å¯èƒ½æ˜¯å› ä¸ºè¾“å…¥è¿‡é•¿.)")
    elif "does not exist" in error_msg:
        chatbot[-1] = (chatbot[-1][0], f"[Local Message] Model {llm_kwargs['llm_model']} does not exist. æ¨¡å‹ä¸å­˜åœ¨, æˆ–è€…æ‚¨æ²¡æœ‰è·å¾—ä½“éªŒèµ„æ ¼.")
    elif "Incorrect API key" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] Incorrect API key. OpenAIä»¥æä¾›äº†ä¸æ­£ç¡®çš„API_KEYä¸ºç”±, æ‹’ç»æœåŠ¡. " + openai_website)
    elif "exceeded your current quota" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] You exceeded your current quota. OpenAIä»¥è´¦æˆ·é¢åº¦ä¸è¶³ä¸ºç”±, æ‹’ç»æœåŠ¡." + openai_website)
    elif "account is not active" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] Your account is not active. OpenAIä»¥è´¦æˆ·å¤±æ•ˆä¸ºç”±, æ‹’ç»æœåŠ¡." + openai_website)
    elif "associated with a deactivated account" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] You are associated with a deactivated account. OpenAIä»¥è´¦æˆ·å¤±æ•ˆä¸ºç”±, æ‹’ç»æœåŠ¡." + openai_website)
    elif "API key has been deactivated" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] API key has been deactivated. OpenAIä»¥è´¦æˆ·å¤±æ•ˆä¸ºç”±, æ‹’ç»æœåŠ¡." + openai_website)
    elif "bad forward key" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] Bad forward key. API2Dè´¦æˆ·é¢åº¦ä¸è¶³.")
    elif "Not enough point" in error_msg:
        chatbot[-1] = (chatbot[-1][0], "[Local Message] Not enough point. API2Dè´¦æˆ·ç‚¹æ•°ä¸è¶³.")
    else:
        from toolbox import regular_txt_to_markdown
        tb_str = '```\n' + trimmed_format_exc() + '```'
        chatbot[-1] = (chatbot[-1][0], f"[Local Message] å¼‚å¸¸ \n\n{tb_str} \n\n{regular_txt_to_markdown(chunk_decoded)}")
    return chatbot, history

def generate_payload(inputs:str, llm_kwargs:dict, history:list, system_prompt:str, image_base64_array:list=[], has_multimodal_capacity:bool=False, stream:bool=True):
    """
    æ•´åˆæ‰€æœ‰ä¿¡æ¯ï¼Œé€‰æ‹©LLMæ¨¡å‹ï¼Œç”Ÿæˆhttpè¯·æ±‚ï¼Œä¸ºå‘é€è¯·æ±‚åšå‡†å¤‡
    """
    from request_llms.bridge_all import model_info

    if not is_any_api_key(llm_kwargs['api_key']):
        raise AssertionError("ä½ æä¾›äº†é”™è¯¯çš„API_KEYã€‚\n\n1. ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼šç›´æ¥åœ¨è¾“å…¥åŒºé”®å…¥api_keyï¼Œç„¶åå›è½¦æäº¤ã€‚\n\n2. é•¿æ•ˆè§£å†³æ–¹æ¡ˆï¼šåœ¨config.pyä¸­é…ç½®ã€‚")

    if llm_kwargs['llm_model'].startswith('vllm-'):
        api_key = 'no-api-key'
    else:
        api_key = select_api_key(llm_kwargs['api_key'], llm_kwargs['llm_model'])

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    if API_ORG.startswith('org-'): headers.update({"OpenAI-Organization": API_ORG})
    if llm_kwargs['llm_model'].startswith('azure-'):
        headers.update({"api-key": api_key})
        if llm_kwargs['llm_model'] in AZURE_CFG_ARRAY.keys():
            azure_api_key_unshared = AZURE_CFG_ARRAY[llm_kwargs['llm_model']]["AZURE_API_KEY"]
            headers.update({"api-key": azure_api_key_unshared})

    if has_multimodal_capacity:
        # å½“ä»¥ä¸‹æ¡ä»¶æ»¡è¶³æ—¶ï¼Œå¯ç”¨å¤šæ¨¡æ€èƒ½åŠ›ï¼š
        # 1. æ¨¡å‹æœ¬èº«æ˜¯å¤šæ¨¡æ€æ¨¡å‹ï¼ˆhas_multimodal_capacityï¼‰
        # 2. è¾“å…¥åŒ…å«å›¾åƒï¼ˆlen(image_base64_array) > 0ï¼‰
        # 3. å†å²è¾“å…¥åŒ…å«å›¾åƒï¼ˆ any([contain_base64(h) for h in history]) ï¼‰
        enable_multimodal_capacity = (len(image_base64_array) > 0) or any([contain_base64(h) for h in history])
    else:
        enable_multimodal_capacity = False

    conversation_cnt = len(history) // 2
    openai_disable_system_prompt = model_info[llm_kwargs['llm_model']].get('openai_disable_system_prompt', False)

    if openai_disable_system_prompt:
        messages = [{"role": "user", "content": system_prompt}]
    else:
        messages = [{"role": "system", "content": system_prompt}]

    if not enable_multimodal_capacity:
        # ä¸ä½¿ç”¨å¤šæ¨¡æ€èƒ½åŠ›
        if conversation_cnt:
            for index in range(0, 2*conversation_cnt, 2):
                what_i_have_asked = {}
                what_i_have_asked["role"] = "user"
                what_i_have_asked["content"] = remove_image_if_contain_base64(history[index])
                what_gpt_answer = {}
                what_gpt_answer["role"] = "assistant"
                what_gpt_answer["content"] = remove_image_if_contain_base64(history[index+1])
                if what_i_have_asked["content"] != "":
                    if what_gpt_answer["content"] == "": continue
                    if what_gpt_answer["content"] == timeout_bot_msg: continue
                    messages.append(what_i_have_asked)
                    messages.append(what_gpt_answer)
                else:
                    messages[-1]['content'] = what_gpt_answer['content']
        what_i_ask_now = {}
        what_i_ask_now["role"] = "user"
        what_i_ask_now["content"] = inputs
        messages.append(what_i_ask_now)
    else:
        # å¤šæ¨¡æ€èƒ½åŠ›
        if conversation_cnt:
            for index in range(0, 2*conversation_cnt, 2):
                what_i_have_asked = {}
                what_i_have_asked["role"] = "user"
                what_i_have_asked["content"] = append_image_if_contain_base64(history[index])
                what_gpt_answer = {}
                what_gpt_answer["role"] = "assistant"
                what_gpt_answer["content"] = append_image_if_contain_base64(history[index+1])
                if what_i_have_asked["content"] != "":
                    if what_gpt_answer["content"] == "": continue
                    if what_gpt_answer["content"] == timeout_bot_msg: continue
                    messages.append(what_i_have_asked)
                    messages.append(what_gpt_answer)
                else:
                    messages[-1]['content'] = what_gpt_answer['content']
        what_i_ask_now = {}
        what_i_ask_now["role"] = "user"
        what_i_ask_now["content"] = []
        what_i_ask_now["content"].append({
            "type": "text",
            "text": inputs
        })
        for image_base64 in image_base64_array:
            what_i_ask_now["content"].append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{image_base64}"
                }
            })
        messages.append(what_i_ask_now)


    model = llm_kwargs['llm_model']
    if llm_kwargs['llm_model'].startswith('api2d-'):
        model = llm_kwargs['llm_model'][len('api2d-'):]
    if llm_kwargs['llm_model'].startswith('aioagi-'):
        model = llm_kwargs['llm_model'][len('aioagi-'):]
        model, _ = read_one_api_model_name(model)
    if llm_kwargs['llm_model'].startswith('vllm-'):
        model = llm_kwargs['llm_model'][len('vllm-'):]
        model, _ = read_one_api_model_name(model)
    if model == "gpt-3.5-random": # éšæœºé€‰æ‹©, ç»•è¿‡openaiè®¿é—®é¢‘ç‡é™åˆ¶
        model = random.choice([
            "gpt-3.5-turbo",
            "gpt-3.5-turbo-16k",
            "gpt-3.5-turbo-1106",
            "gpt-3.5-turbo-0613",
            "gpt-3.5-turbo-16k-0613",
            "gpt-3.5-turbo-0301",
        ])

    payload = {
        "model": model,
        "messages": messages,
        "temperature": llm_kwargs['temperature'],  # 1.0,
        "top_p": llm_kwargs['top_p'],  # 1.0,
        "n": 1,
        "stream": stream,
    }
    openai_force_temperature_one = model_info[llm_kwargs['llm_model']].get('openai_force_temperature_one', False)
    if openai_force_temperature_one:
        payload.pop('temperature')
    return headers,payload

